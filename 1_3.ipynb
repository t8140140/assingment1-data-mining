{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import random    \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings_table = pd.read_csv(filepath_or_buffer='./ml-1m/ratings.dat',\n",
    "                            sep='::', delimiter=None, header=0, engine='python')\n",
    "\n",
    "# 1.\n",
    "mean_rating_global = ratings_table['Rating'].mean()\n",
    "# 2.\n",
    "mean_rating_per_movie = ratings_table.groupby('MovieID')['Rating'].mean()\n",
    "# .3\n",
    "mean_rating_per_user = ratings_table.groupby('UserID')['Rating'].mean()\n",
    "\n",
    "\n",
    "def generate_X_set(columns, *features):\n",
    "    matrix = np.vstack(features[:2]).T\n",
    "    X_set = pd.DataFrame(data=matrix, columns=columns)\n",
    "    return X_set\n",
    "    \n",
    "X = generate_X_set(['UserID', 'MovieID'],*[ratings_table['UserID'], ratings_table['MovieID']])\n",
    "\n",
    "X_per_movie = generate_X_set(['MovieID', 'UserID'],*[ratings_table['UserID'], ratings_table['MovieID'], mean_rating_per_movie])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_global = X.copy()\n",
    "\n",
    "X_global['global_average'] = mean_rating_global\n",
    "\n",
    "X_per_user = pd.merge(left=X, right=mean_rating_per_user, how='left', \n",
    "              left_on='UserID', right_index=True)\n",
    "\n",
    "X_per_movie = pd.merge(left=X, right=mean_rating_per_movie, how='left', \n",
    "              left_on='MovieID', right_index=True)\n",
    "\n",
    "X_per_user_and_movie = pd.merge(left=X_per_user, right=mean_rating_per_movie, how='left', \n",
    "              left_on='MovieID', right_index=True)\n",
    "\n",
    "y = ratings_table['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "def k_fold_split_training(X, y, intercept=False): \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=32)\n",
    "    rmse_test = []\n",
    "    rmse_train = []\n",
    "    \n",
    "    mae_test = []\n",
    "    mae_train = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        linear_reg = LinearRegression(fit_intercept=intercept)\n",
    "\n",
    "        linear_reg.fit(X_train, y_train)\n",
    "        \n",
    "        y_predict_test = linear_reg.predict(X_test)\n",
    "        y_predict_train = linear_reg.predict(X_train)\n",
    "        \n",
    "        rmse_train.append(np.sqrt(mean_squared_error(y_train, y_predict_train)))\n",
    "        rmse_test.append(np.sqrt(mean_squared_error(y_test, y_predict_test)))\n",
    "        mae_train.append(mean_absolute_error(y_train, y_predict_train))\n",
    "        mae_test.append(mean_absolute_error(y_test, y_predict_test))\n",
    "\n",
    "    return np.sum(rmse_test)/5, np.sum(rmse_train)/5, np.sum(mae_test)/5, np.sum(mae_train)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m total_maes_train\u001b[39m.\u001b[39mappend(mae_train)\n\u001b[0;32m     10\u001b[0m total_maes_test\u001b[39m.\u001b[39mappend(mae_test)\n\u001b[1;32m---> 11\u001b[0m rmse_test, rmse_train, mae_test, mae_train \u001b[39m=\u001b[39m k_fold_split_training(X_per_user, y)\n\u001b[0;32m     12\u001b[0m total_rmses_train\u001b[39m.\u001b[39mappend(rmse_train)\n\u001b[0;32m     13\u001b[0m total_rmses_test\u001b[39m.\u001b[39mappend(rmse_test)\n",
      "Cell \u001b[1;32mIn [22], line 17\u001b[0m, in \u001b[0;36mk_fold_split_training\u001b[1;34m(X, y, intercept)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(train_index))\n\u001b[0;32m     16\u001b[0m X_train, X_test \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc[train_index], X\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m---> 17\u001b[0m y_train, y_test \u001b[39m=\u001b[39m y[train_index], y[test_index]\n\u001b[0;32m     19\u001b[0m linear_reg \u001b[39m=\u001b[39m LinearRegression(fit_intercept\u001b[39m=\u001b[39mintercept)\n\u001b[0;32m     21\u001b[0m linear_reg\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values(key)\n\u001b[1;32m-> 1007\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_with(key)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1042\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[39mif\u001b[39;00m key_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minteger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1039\u001b[0m     \u001b[39m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m-> 1042\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[key]\n\u001b[0;32m   1043\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[key]\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1434\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6107\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m   6106\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer_for(keyarr)\n\u001b[1;32m-> 6107\u001b[0m     keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex(keyarr)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   6108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6109\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4455\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4453\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4454\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 4455\u001b[0m         indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_indexer(\n\u001b[0;32m   4456\u001b[0m             target, method\u001b[39m=\u001b[39;49mmethod, limit\u001b[39m=\u001b[39;49mlimit, tolerance\u001b[39m=\u001b[39;49mtolerance\n\u001b[0;32m   4457\u001b[0m         )\n\u001b[0;32m   4458\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi:\n\u001b[0;32m   4459\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot handle a non-unique multi-index!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3974\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3969\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3970\u001b[0m     \u001b[39mreturn\u001b[39;00m this\u001b[39m.\u001b[39m_get_indexer(\n\u001b[0;32m   3971\u001b[0m         target, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance\n\u001b[0;32m   3972\u001b[0m     )\n\u001b[1;32m-> 3974\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indexer(target, method, limit, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:423\u001b[0m, in \u001b[0;36mRangeIndex._get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m    420\u001b[0m locs[\u001b[39m~\u001b[39mvalid] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    421\u001b[0m locs[valid] \u001b[39m=\u001b[39m locs[valid] \u001b[39m/\u001b[39m step\n\u001b[1;32m--> 423\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep:\n\u001b[0;32m    424\u001b[0m     \u001b[39m# We reversed this range: transform to original locs\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     locs[valid] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m locs[valid]\n\u001b[0;32m    426\u001b[0m \u001b[39mreturn\u001b[39;00m ensure_platform_int(locs)\n",
      "File \u001b[1;32mc:\\Users\\cheemo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:293\u001b[0m, in \u001b[0;36mRangeIndex.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    287\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deprecation_message\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m_stop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    288\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    289\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    290\u001b[0m     )\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop\n\u001b[1;32m--> 293\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39m    The value of the `step` parameter (``1`` if this was not supplied).\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[39m# GH 25710\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rmses_train = []\n",
    "total_rmses_test = []\n",
    "total_maes_train = []\n",
    "total_maes_test = []\n",
    "\n",
    "rmse_test, rmse_train, mae_test, mae_train = k_fold_split_training(X_global, y)\n",
    "total_rmses_train.append(rmse_train)\n",
    "total_rmses_test.append(rmse_test)\n",
    "total_maes_train.append(mae_train)\n",
    "total_maes_test.append(mae_test)\n",
    "rmse_test, rmse_train, mae_test, mae_train = k_fold_split_training(X_per_user, y)\n",
    "total_rmses_train.append(rmse_train)\n",
    "total_rmses_test.append(rmse_test)\n",
    "total_maes_train.append(mae_train)\n",
    "total_maes_test.append(mae_test)\n",
    "rmse_test, rmse_train, mae_test, mae_train = k_fold_split_training(X_per_movie, y)\n",
    "total_rmses_train.append(rmse_train)\n",
    "total_rmses_test.append(rmse_test)\n",
    "total_maes_train.append(mae_train)\n",
    "total_maes_test.append(mae_test)\n",
    "rmse_test, rmse_train, mae_test, mae_train = k_fold_split_training(X_per_user_and_movie, y)\n",
    "total_rmses_train.append(rmse_train)\n",
    "total_rmses_test.append(rmse_test)\n",
    "total_maes_train.append(mae_train)\n",
    "total_maes_test.append(mae_test)\n",
    "rmse_test, rmse_train, mae_test, mae_train = k_fold_split_training(X_per_user_and_movie, y, True)\n",
    "total_rmses_train.append(rmse_train)\n",
    "total_rmses_test.append(rmse_test)\n",
    "total_maes_train.append(mae_train)\n",
    "total_maes_test.append(mae_test)\n",
    "\n",
    "print('rmse train: ', total_rmses_train)\n",
    "print('rmse test: ', total_rmses_test)\n",
    "print('mae train: ', total_maes_train)\n",
    "print('mae test: ', total_maes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_u_v(m):\n",
    "    u = np.full((m.shape[0], 2), 1)\n",
    "    v = np.full((2, m.shape[1]), 1)\n",
    "    u = u.astype(np.float32)\n",
    "    v = v.astype(np.float32)\n",
    "    return u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(m):\n",
    "    Row_df = m.pivot(index='user_id', columns='movie_id', values='rating')\n",
    "    u_mean = Row_df.mean(axis=1)\n",
    "    Row_df_array = np.array(Row_df)\n",
    "    u_mean = np.array(u_mean)\n",
    "    # creating a normal matrix to compare to our uv matrix\n",
    "    print(u_mean)\n",
    "    print(u_mean.reshape(-1, 1))\n",
    "    normal = Row_df_array - u_mean.reshape(-1, 1)\n",
    "    N = normal\n",
    "    return N, Row_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_v(u, v, N, r, s):\n",
    "    sums = 0\n",
    "    u_ik = u[:, :]\n",
    "    v_ks = v[:, s]\n",
    "    u_ik_del = np.delete(u_ik, r, 1)\n",
    "    v_ks_del = np.delete(v_ks, r, 0)\n",
    "    u_ir = u[:, r]\n",
    "    u_ir_squared = u_ir ** 2\n",
    "    u_ik_v_ks = np.dot(u_ik_del, v_ks_del)\n",
    "    m_is = N[:, s]\n",
    "    error = m_is - u_ik_v_ks\n",
    "    uir_dot_er = u_ir * error\n",
    "    sumsv = np.nansum(uir_dot_er)\n",
    "    u_ir_ssum = np.nansum(u_ir_squared * (~np.isnan(m_is)))\n",
    "    newval_v = sumsv / u_ir_ssum\n",
    "    v[r, s] = v[r, s] + ((newval_v - v[r, s]))\n",
    "    return u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_u(u, v, N, r, s):\n",
    "    sums = 0\n",
    "    u_rk = u[r, :]\n",
    "    v_kj = v[:, :]\n",
    "\n",
    "    # to calculate the part of the matrices not affected by the value at index r\n",
    "    u_rk_del = np.delete(u_rk, s, 0)\n",
    "    v_kj_del = np.delete(v_kj, s, 0)\n",
    "    v_sj = v[s, :]\n",
    "    v_sj_squared = v_sj ** 2\n",
    "    # create the matrix combination of u and v which would be subtracted from original matrix for error\n",
    "    u_rk_v_kj = np.dot(u_rk_del, v_kj_del)\n",
    "    m_rj = N[r, :]\n",
    "    error = m_rj - u_rk_v_kj\n",
    "    vsj_dot_er = v_sj * error\n",
    "    sums = np.nansum(vsj_dot_er)\n",
    "    v_sj_ssum = np.nansum((v_sj_squared) * (~np.isnan(m_rj)))\n",
    "    newval_u = sums / v_sj_ssum\n",
    "    u[r, s] = u[r, s] + ((newval_u - u[r, s]))\n",
    "    return u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(dif):\n",
    "    dif_abs = (np.absolute(dif))\n",
    "    # converting all nan values to a zero value.\n",
    "    dif_abs_0s = np.nan_to_num(dif_abs)\n",
    "    dif_abs_sum = np.sum(dif_abs_0s, axis=0)\n",
    "    sum_dif = dif_abs_sum.sum()\n",
    "    non_0_count = np.count_nonzero(dif_abs_0s)\n",
    "    MAE = sum_dif/non_0_count\n",
    "    return MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(dif):\n",
    "    dif_sqr = dif ** 2\n",
    "    dif_sqr_0s = np.nan_to_num(dif_sqr)\n",
    "    dif_sqr_total = np.sum(dif_sqr_0s, axis=0)\n",
    "    sumz = dif_sqr_total.sum()\n",
    "    non_0_count_sqr = np.count_nonzero(dif_sqr_0s)\n",
    "    RMSE = sumz / non_0_count_sqr\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheemo\\AppData\\Local\\Temp\\ipykernel_28448\\1524918027.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  u_mean = Row_df.mean(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " ...\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m RT_train, RT_test \u001b[39m=\u001b[39m RT\u001b[39m.\u001b[39mloc[train_index], RT\u001b[39m.\u001b[39mloc[test_index]\n\u001b[0;32m     15\u001b[0m \u001b[39m# create a dataframe to store all ratings as values for each movie in a coloumn with every user id as index of the rows.\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m normal, Row_df \u001b[39m=\u001b[39m normalize_data(RT_test)\n\u001b[0;32m     17\u001b[0m N \u001b[39m=\u001b[39m normal\n\u001b[0;32m     18\u001b[0m Row_df_array \u001b[39m=\u001b[39m Row_df\u001b[39m.\u001b[39mto_numpy()\n",
      "Cell \u001b[1;32mIn [21], line 9\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(u_mean)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(u_mean\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m normal \u001b[39m=\u001b[39m Row_df_array \u001b[39m-\u001b[39;49m u_mean\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m N \u001b[39m=\u001b[39m normal\n\u001b[0;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m N, Row_df\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# UV Decomposition - Test\n",
    "\n",
    "# input the path of ratings.dat file\n",
    "RT = pd.read_csv('./ml-1m/ratings.dat', engine='python', sep='::',\n",
    "                 names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "\n",
    "# create a kfold function to divide the data into 5 random sets for cross validation\n",
    "KF = KFold(n_splits=5, shuffle=True, random_state=9)\n",
    "c = 2\n",
    "i = 5\n",
    "\n",
    "# start the iteration for each of the 5 folds\n",
    "for train_index, test_index in KF.split(RT):\n",
    "    RT_train, RT_test = RT.loc[train_index], RT.loc[test_index]\n",
    "    # create a dataframe to store all ratings as values for each movie in a coloumn with every user id as index of the rows.\n",
    "    normal, Row_df = normalize_data(RT_test)\n",
    "    N = normal\n",
    "    Row_df_array = Row_df.to_numpy()\n",
    "    # creating uv matrix components with u having n X d and v having d X m ( where n = number of users, m = number of movies and d = 2)\n",
    "    u, v = create_u_v(normal)\n",
    "    uv = np.dot(u, v)\n",
    "    print(\"Index:\", test_index)\n",
    "  # updating u using the formula x =(Σj vsj (mrj−Σk̸=surkvkj))/Σjv^2sj\n",
    "    for iterations in range(i):\n",
    "        for r in range(len(RT_test)):\n",
    "            for s in range(c):\n",
    "                u, v = update_u(u, v, N, r, s)\n",
    "        # update v using the formula y = (Σiuir(mis−Σk̸=ruikvks))/Σiu^2ir\n",
    "        for r in range(c):\n",
    "            for s in range(Row_df_array.shape[1]):\n",
    "                u, v = update_v(u, v, N, r, s)\n",
    "        uv = np.dot(u, v)\n",
    "        dif = uv-normal\n",
    "        print(\"Iteration Number: \", iterations)\n",
    "        MAE = mae(dif)\n",
    "        print('MAE', MAE)\n",
    "        # calculating RMSE\n",
    "        RMSE = rmse(dif)\n",
    "        print('RMSE=', RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_of_iterations = 75\n",
    "reguralization_factor = 0.05\n",
    "num_of_factors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_table = pd.read_csv(filepath_or_buffer='./ml-1m/ratings.dat',\n",
    "                            sep='::', delimiter=None, header=0, names=['user_id', 'movie_id', 'rating', 'timestamp'] ,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import isnan\n",
    "\n",
    "def create_matrices(ratings):\n",
    "    \n",
    "    user_ids = ratings['user_id'].unique().tolist()\n",
    "    movie_ids = ratings['movie_id'].unique().tolist()\n",
    "\n",
    "    num_users = len(user_ids)\n",
    "    num_movies = len(movie_ids)\n",
    "\n",
    "    idx_user = range(num_users)\n",
    "    idx_movie = range(num_movies)\n",
    "\n",
    "    mapping_user = dict(zip(user_ids, idx_user))\n",
    "    mapping_movie = dict(zip(movie_ids, idx_movie))\n",
    "    \n",
    "    X = np.array(ratings.pivot(index='user_id',columns='movie_id', values='rating'))\n",
    "    U = np.random.uniform(-0.01, 0.01, (num_users, num_of_factors))\n",
    "    M = np.random.uniform(-0.01, 0.01, (num_of_factors, num_movies))\n",
    "\n",
    "    return [X, U, M, mapping_user, mapping_movie]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_der_reg(error, element):\n",
    "    deriv_reg = 2*error - reguralization_factor*element\n",
    "    return deriv_reg\n",
    "\n",
    "\n",
    "def calculate_predictions(X, U, M):\n",
    "    previous_rmse = 100000\n",
    "\n",
    "    for _ in range(num_of_iterations):\n",
    "        total_errors = []\n",
    "        for i, xi in enumerate(X):\n",
    "            for j, xj in enumerate(X[i]):\n",
    "\n",
    "                if isnan(X[i, j]):\n",
    "                    continue\n",
    "\n",
    "                pred_xij = np.dot(U[i, :], M[:, j])\n",
    "\n",
    "                error_xij = X[i, j] - pred_xij\n",
    "\n",
    "                total_errors.append(error_xij)\n",
    "\n",
    "                for k in range(num_of_factors):\n",
    "                    Uik = U[i, k] + learning_rate * \\\n",
    "                        (partial_der_reg(error_xij, M[k, j]))\n",
    "\n",
    "                    Mkj = M[k, j] + learning_rate * \\\n",
    "                        (partial_der_reg(error_xij, U[i, k]))\n",
    "\n",
    "                    U[i, k] = Uik\n",
    "                    M[k, j] = Mkj\n",
    "    \n",
    "        rmse = np.sqrt(np.sum(np.array(total_errors)**2))/len(total_errors)\n",
    "        if rmse == previous_rmse:\n",
    "            break\n",
    "\n",
    "        previous_rmse = rmse\n",
    "\n",
    "    return [U, M]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(predictions, actual):\n",
    "    total_errors = []\n",
    "    for i in range(len(actual)):\n",
    "        for j in range(len(actual[i])):\n",
    "            if not isnan(actual[i][j]):\n",
    "                error = predictions[i][j] - actual[i][j]\n",
    "                total_errors.append(error)\n",
    "                \n",
    "    return np.sqrt(np.sum(np.array(total_errors)**2))/len(total_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_test(predictions, actual, user_train_mapping, movie_train_mapping, user_test_mapping, movie_test_mapping):\n",
    "    total_errors = []\n",
    "    for u_id, u_index in user_test_mapping.items():\n",
    "      for m_id, m_index in movie_test_mapping.items():\n",
    "            if u_id in user_train_mapping and m_id in movie_train_mapping:\n",
    "                if not isnan(actual[u_index][m_index]):\n",
    "                    error = predictions[user_train_mapping[u_id]][movie_train_mapping[m_id]] - actual[u_index][m_index]\n",
    "                    total_errors.append(error)\n",
    "\n",
    "    return np.sqrt(np.sum(np.array(total_errors)**2))/len(total_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_data(Users, Movies):\n",
    "    predictions = np.matmul(Users, Movies)\n",
    "    \n",
    "    predictions[predictions > 5] = 5\n",
    "    predictions[predictions < 1] = 1\n",
    "     \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_matrix_factorization(data, learning_rate, iterations, regularization, num_factors):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    rmse_train_total = []\n",
    "    rmse_test_total = []\n",
    "    previous_test_rmse = 10000\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train_data = data.iloc[train_index]\n",
    "        test_data = data.iloc[test_index]\n",
    "        \n",
    "        X_train, U_train, M_train, user_train_mapping, movie_train_mapping = create_matrices(train_data)\n",
    "        \n",
    "        X_test, U_test, M_test, user_test_mapping, movie_test_mapping = create_matrices(test_data)\n",
    "\n",
    "        U_predicted, M_predicted = calculate_predictions(\n",
    "            X_train, U_train, M_train)\n",
    "\n",
    "        P_predicted = post_process_data(U_predicted, M_predicted)\n",
    "\n",
    "        rmse_train = calculate_rmse(P_predicted, X_train)\n",
    "        rmse_test = calculate_rmse_test(P_predicted, X_test, user_train_mapping, movie_train_mapping, user_test_mapping, movie_test_mapping)\n",
    "\n",
    "        if rmse_test < previous_test_rmse:\n",
    "            previous_test_rmse = rmse_test\n",
    "            U_best, M_best = U_predicted, M_predicted\n",
    "            user_train_mapping_best, movie_train_mapping_best = user_train_mapping, movie_train_mapping #maximos\n",
    "\n",
    "        rmse_train_total.append(rmse_train)\n",
    "        rmse_test_total.append(rmse_test)\n",
    "\n",
    "    return [U_best, M_best, rmse_train_total, rmse_test_total, user_train_mapping_best, movie_train_mapping_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, M, rmse_train, rmse_test, user_train_mapping_best, movie_train_mapping_best = k_fold_matrix_factorization(ratings_table, learning_rate, num_of_iterations, reguralization_factor, num_of_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3676"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_train_indices = []\n",
    "for k, v in user_train_mapping_best.items():\n",
    "    user_train_indices.append(int(v))\n",
    "\n",
    "movie_train_indices = []\n",
    "for k, v in movie_train_mapping_best.items():\n",
    "    movie_train_indices.append(int(v))\n",
    "\n",
    "np.savetxt('user_train_indices.csv', user_train_indices, delimiter=',', fmt='%d')\n",
    "np.savetxt('movie_train_indices.csv', movie_train_indices, delimiter=',', fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67c387c2c5b8063b0f5207e4f2f5d1ecdb3788cfb68a74a0e938e4e5a4672a70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
